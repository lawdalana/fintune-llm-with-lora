{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90a9d9-f488-4149-88e8-a9b7dcfb2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install packaging\n",
    "# !pip install ninja\n",
    "# !pip install flash-attn --no-build-isolation\n",
    "# https://github.com/bdashore3/flash-attention/releases\n",
    "# !pip install peft transformers datasets\n",
    "# https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/5-Fine%20Tuning/LoRA_Tuning_PEFT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1bcae-b951-4eff-84dd-1ccb274e15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python      3.11.7\n",
    "# GPU         4070TI 12GB\n",
    "# Cuda        cuda_12.1.r12.1\n",
    "# Library\n",
    "# torch       2.2.2+cu121\n",
    "# flash_attn  2.5.9.post1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3480ea2-4273-4596-95d5-7a9571bb97b5",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b155bc6-711f-4756-9d5d-221a598e00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_model, \n",
    "    LoraConfig, \n",
    "    TaskType, \n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import transformers\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58e1fc-6fe2-4f4b-b9f4-df007e0cb6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca207be4-27b3-49a8-8679-0dcc55bcc135",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_threshold=6.0,\n",
    "        # llm_int8_skip_modules=None,\n",
    "        # llm_int8_enable_fp32_cpu_offload=False,\n",
    "        # llm_int8_has_fp16_weight=False,\n",
    "    \n",
    "        # load_in_4bit=True,\n",
    "        # bnb_4bit_quant_type='nf4',\n",
    "        # bnb_4bit_compute_dtype=compute_dtype,\n",
    "        # bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfb306-24cb-4d65-923c-b8cead49c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", \n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_cache=False\n",
    ") # load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41379233-b454-49f9-8f89-8a0dc9c68f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1bc55-7cdf-43ec-98d1-fa71e5b95e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06623f-e092-4804-9184-64a8f37592b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=32, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1,\n",
    "    # target_modules='all-linear'\n",
    "    target_modules=[\"qkv_proj\"] # optional, you can target specific layers using this\n",
    "    # target_modules=[\"v_proj\", \"q_proj\"]\n",
    ") # create LoRA config for the finetuning\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config) # create a model ready for LoRA finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c1fba-9278-4adb-8d89-1ae7add7f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c416554-2fdc-49b6-b812-20b674654121",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.print_trainable_parameters() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16140398-4c16-4e75-bc45-08ba9a2d0d69",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ccf46f-95e0-4a71-a413-01ade43c5ae7",
   "metadata": {},
   "source": [
    "## Load and Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ceb44-c3fa-404c-908c-65d7efdce277",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/full_formal_script_temp0.8.pkl', 'rb') as fp:\n",
    "    data_formal = pickle.load(fp)\n",
    "\n",
    "with open('./dataset/full_informal_script_temp0.8.pkl', 'rb') as fp:\n",
    "    data_informal = pickle.load(fp)\n",
    "\n",
    "with open('./dataset/full_novel_script_temp0.8.pkl', 'rb') as fp:\n",
    "    data_novel = pickle.load(fp)\n",
    "\n",
    "df = pd.read_csv('./dataset/raw.csv', index_col='uid')\n",
    "# https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf904ab-de0b-4416-8aa4-37b5406b70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['json'] = df.apply(lambda x : {'name': x['name'], 'age': x['age'], 'job': x['job']}, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cacf92d-0f6b-4d66-a7d0-ed2f92d0e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in data_formal:\n",
    "    for uid in data_formal[temp]:\n",
    "        df.loc[uid, 'formal'] = data_formal[temp][uid]\n",
    "\n",
    "for temp in data_informal:\n",
    "    for uid in data_informal[temp]:\n",
    "        df.loc[uid, 'informal'] = data_informal[temp][uid]\n",
    "        \n",
    "for temp in data_novel:\n",
    "    for uid in data_novel[temp]:\n",
    "        df.loc[uid, 'novel'] = data_novel[temp][uid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db56af6-9699-4cf1-9668-d2fff3d9c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "index = int(len(df)*train_ratio)\n",
    "train_df, test_df = df[:index], df[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1186254-077c-4578-808b-65cad26bb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prep_df = train_df.reset_index()[['uid', 'json', 'formal', 'informal', 'novel']].melt(\n",
    "    id_vars=['uid','json'],\n",
    "    var_name=\"type\",\n",
    "    value_name=\"context\"\n",
    ").sort_values('uid')\n",
    "train_prep_df = train_prep_df[['json', 'context']]\n",
    "\n",
    "test_prep_df = test_df.reset_index()[['uid', 'json', 'formal', 'informal', 'novel']].melt(\n",
    "    id_vars=['uid','json'],\n",
    "    var_name=\"type\",\n",
    "    value_name=\"context\"\n",
    ").sort_values('uid')\n",
    "test_prep_df = test_prep_df[['json', 'context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88194fdc-7a62-48fa-bbf6-6ffe428d08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/67852880/how-can-i-handle-this-datasets-to-create-a-datasetdict\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_prep_df, preserve_index=False),\n",
    "    'test': Dataset.from_pandas(test_prep_df, preserve_index=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3769f-44fc-434c-8771-a65a9e9b66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83656654-44c0-46d6-bf60-1cf41d554bae",
   "metadata": {},
   "source": [
    "## Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2ddf3-e299-4a10-b0b1-cfe553418cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_fn = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d852e3-fc1d-4afe-872a-14de13a9217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample, add_result=False):\n",
    "    ################# Version = 1 ################\n",
    "#     prompt = f\"\"\"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.\n",
    "# Extract name, age, job from the sentence to json format. if the information doesn't exsits fill null.\n",
    "# Sentence: '{sample['context']}'\n",
    "# \"\"\"\n",
    "#     if add_result:\n",
    "#         prompt += f\"{sample['json']}\"\n",
    "\n",
    "    ################# Version = 2 ################\n",
    "    \n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.<|end|>\n",
    "<|user|>\n",
    "Extract name, age, job from the sentence to json format. if the information doesn't exsits fill null.\n",
    "\n",
    "Sentence: '{sample['context']}'<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    prompt += f\"{sample['json']}<|end|>\"\n",
    "    sample[\"text\"] = prompt\n",
    "    # sample[\"json\"] = str(sample['json'])\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c305dce-27d3-45af-9cfb-9cd0ff54c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train'].map(create_prompt_formats, fn_kwargs={'add_result': True}, remove_columns=['json', 'context'])\n",
    "eval_dataset = dataset['test'].map(create_prompt_formats, fn_kwargs={'add_result': False}, remove_columns=['json', 'context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56195d2-830b-4be1-a902-84f1049fde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6682b1d1-0c6f-4673-b626-530e5ac80f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return token_fn(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d75aa2-0d3f-4bc9-96b8-3de17b58044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55a361-2a7e-41fa-addf-94c268b38c75",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cdd2ad-10c7-4153-b233-49d60ff6718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "model_to_inspect = peft_model.get_base_model()\n",
    "signature = inspect.signature(model_to_inspect.forward)\n",
    "list(signature.parameters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487c8d3-7fed-4c68-a46d-987037f6d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./Phi-3-mini-4k-instruct-8Blora-text2json-training-clean-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps=10,\n",
    "    save_total_limit=50,\n",
    "    logging_steps=1,\n",
    "    \n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d0671-f45f-4cfa-87be-c13f4de6f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(token_fn, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50d1a2-7784-4061-89e6-44615b426c59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    print(\"Attempting to continue training on CPU...\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    training_args.fp16 = False\n",
    "    training_args.per_device_train_batch_size = 1\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebc3ac",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9a493-b5c6-4e29-bd7e-a96856e492c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained('./Tuning/checkpoint/Phi-3-mini-4k-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9d488-57ca-4284-99eb-95bd535775d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_fn.save_pretrained('./Tuning/checkpoint/Phi-3-mini-4k-instruct/tokenize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edc862",
   "metadata": {},
   "source": [
    "## Test Call finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadd849-b776-4e60-952d-b7cb4e01969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats_eval(sample):\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.<|end|>\n",
    "<|user|>\n",
    "Extract name, age, job from the sentence to json format. if the information doesn't exsits fill null.\n",
    "\n",
    "Sentence: '{sample['context']}'<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    sample[\"text\"] = prompt\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd2a77-d813-4ba7-ad22-c91c60d8c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "\n",
    "inputs = token_fn(\n",
    "    eval[index]['text'], \n",
    "    truncation=True, \n",
    "    padding=\"max_length\", \n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ").to(peft_model.device)\n",
    "        \n",
    "# Generate the prediction\n",
    "outputs = peft_model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "# # Decode the output\n",
    "predicted_text = token_fn.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "result = re.findall('{.*}', predicted_text)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
